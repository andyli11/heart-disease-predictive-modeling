{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "## Predictive Modeling of Heart Disease\n\n### Acknowledgement of Dataset\n**Heart Disease UCI:** [https://archive.ics.uci.edu/ml/datasets/Heart+Disease](https://archive.ics.uci.edu/ml/datasets/Heart+Disease)\n\n**Database Source:**\n\nDua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of\nInformation and Computer Science. \n\n**Database Creators:**\n- Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\n- University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\n- University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\n- V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.\n\n### Features\n\n| Feature                | Description                                                | Units        |\n|------------------------|------------------------------------------------------------|--------------|\n| age                    | Age                                                        | Years        |\n| sex                    | Sex                                                        | -            |\n| cp                     | Chest pain type (4 values)                                 | -            |\n| trestbps               | Resting blood pressure (in mm Hg on admission to the hospital) | mm Hg        |\n| chol                   | Serum cholesterol                                          | mg/dl        |\n| fbs                    | Fasting blood sugar > 120 mg/dl (1 = true; 0 = false)      | -            |\n| restecg                | Resting electrocardiographic results (values 0 = normal, 1, 2) | -            |\n| thalach                | Maximum heart rate achieved                                | bpm          |\n| exang                  | Exercise induced angina (1 = yes; 0 = no)                  | -            |\n| oldpeak                | ST depression induced by exercise relative to rest         | -            |\n| slope                  | The slope of the peak exercise ST segment (values 1 = upsloping, 2 = flat, 3 = downsloping) | -            |\n| ca                     | Number of major vessels (0-3) colored by fluoroscopy       | -            |\n| thal                   | 3 = normal; 6 = fixed defect; 7 = reversible defect        | -            |\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "%pip install seaborn",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''\nImport pandas, NumPy, Matplotlib and Seaborn.\nImport the following from sklearn:\n  - metrics and svm;\n  - GaussianNB from naive_bayes;\n  - confusion_matrix, plot_confusion_matrix, classification_report from metrics\n  - LogisticRegression from linear_model\n'''\n\n%matplotlib inline\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\n\nprint('Libraries have been imported.')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Read in the dataset heart.csv using pandas\n\nheart = pd.read_csv('heart.csv')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Read first 5 rows of the dataset\n\nheart.head()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Rename the columns for better readability\n\ncolumns_names = {'cp':'chest_pain_type',\n                 'trestbps':'resting_blood_pressure',\n                 'exang':'exercise_ang',\n                 'chol': 'serum_cholesterol', \n                 'fbs': 'fasting_blood_sugar',\n                 'exang': 'exercise_ang',\n                 'thal': 'max_heart_rate'}\n\n# pandas rename() method and input columns_names to it\n\nheart.rename(columns=columns_names, inplace=True)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Display the updated datatypes for all the features\n\nheart.info()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Check for NaN values\n\nheart.isna().any()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Check the number of missing values per feature\n\nheart.isna().sum()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Visualization\nNow that the dataset has been cleaned, let's visualize it.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Plot the distribution of age feature in the dataset using Seaborn\n# use Seaborn's distplot() method and specify a number of bins\n\nsns.set_style(style='whitegrid')\nsns.histplot(heart['age'], color='red', bins=25, kde=True)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The plot shows that a high percentage of the observations in this dataset are for people aged between 50 and 60.\nBefore implementing any models, we define the variables, and then split the data into training and test sets.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Define variables: X is everything but target; y is target.\n\nX = heart.drop('target', axis=1)\ny = heart['target']",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Display the first few X entries and compare to the cleaned dataset\n\nX.head()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Display the first few y entries and compare to the cleaned dataset\n\ny.head()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Split data to training and test sets with split 70-30\n# Use random_state = 42\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "# Building Models\nRun Logistic Regression, SVM, and Naive Bayes classifiers and obtain the following for every model:\n1. Confusion matrix\n2. Accuracy score\n3. Classification report",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Logistic Regression",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Run Logistic Regression model\n\nlogreg = LogisticRegression(solver='liblinear', max_iter=1000)\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\n\ny_pred, y_pred.shape, y_test.shape, X_test.shape",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Obtain and plot the confusion matrix\n\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['no disease', 'heart disease'])\ndisp.plot(cmap=plt.cm.Reds)\ndisp.ax_.set_title('Confusion Matrix')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Confusion matrix shows 32 correct negatives and 42 correct positives.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Compute the accuracy score\nlogreg_s = logreg.score(X_test, y_test)\nprint('Accuracy:', logreg_s*100, '%')\n\n# Obtain the classification report\nprint(classification_report(y_test, y_pred, labels=[0,1], target_names=['no disease', 'heart disease']))\n\n# Obtain the precision score",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "**Logistic Regression Results:**\n- Accuracy score is approximately 0.813, meaning that 81.3% of the observations were correctly classified (as disease or no_disease).\n- Precision for \"no disease\" is 0.80, meaning that out of all the instances predicted as \"no disease,\" 80% were correctly predicted.\n- Precision for \"heart disease\" is 0.82, meaning that out of all the instances predicted as \"heart disease,\" 82% were correctly predicted.\n\nTherefore, the model is performing reasonably well, but we need to compare those metrics with the other model's metrics to decide which of the\nthree models performs better on the given dataset.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# SVM Classifier",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Run SVM model and obtain the predictions\n# create SVM classifier\nsvm_model = svm.SVC(kernel='linear')\n\n# train the model using the training sets\nsvm_model.fit(X_train, y_train)\n\n# predict the response for test dataset\ny_pred = svm_model.predict(X_test)\ny_pred, y_pred.shape, y_test.shape, X_test.shape",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Obtain and plot the confusion matrix\n\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['no disease', 'heart disease'])\ndisp.plot(cmap=plt.cm.Reds)\nplt.title('Confusion Matrix')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Obtain the accuracy score\nsvm_model_s = svm_model.score(X_test, y_test)\nprint('Accuracy:', svm_model_s*100, '%')\n\n# Obtain the classification report\nprint(classification_report(y_test, y_pred, labels=[0,1], target_names=['no disease', 'heart disease']))\n\n# Obtain the Precision Score",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "**SVM Classifier Results:**\n- Accuracy score is approximately 0.813, meaning that 81.3% of the observations were correctly classified (as disease or no_disease).\n- Precision for \"no disease\" is 0.80, meaning that out of all the instances predicted as \"no disease,\" 80% were correctly predicted.\n- Precision for \"heart disease\" is 0.82, meaning that out of all the instances predicted as \"heart disease,\" 82% were correctly predicted.\n\nThose values are very close to the metric values obtained by the Logistic Regression model which means both models have the same performance.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Naive Bayes Classifier",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Run the Naive Bayes algorithm\n# create a Gaussian Classifier\ngnb_model = GaussianNB()\n\n# train the model using the training sets\ngnb_model.fit(X_train, y_train)\n\n# predict the reponse for test dataset\ny_pred = gnb_model.predict(X_test)\ny_pred, y_pred.shape, y_test.shape, X_test.shape",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Obtain and plot the confusion matrix\n\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['no disease', 'heart disease'])\ndisp.plot(cmap=plt.cm.Reds)\nplt.title('Confusion Matrix')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Obtain the accuracy score\ngnb_model_s = gnb_model.score(X_test, y_test)\nprint('Accuracy:', gnb_model_s*100, '%')\n\n# Obtain the classification report\nprint(classification_report(y_test, y_pred, labels=[0,1], target_names=['no disease', 'heart disease']))\n\n# Obtain the Precision Score",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "**Naive Bayes Classifier Results:**\n- Accuracy score is approximately 0.835, meaning that 83.5% of the observations were correctly classified (as disease or no_disease).\n- Precision for \"no disease\" is 0.78, meaning that out of all the instances predicted as \"no disease,\" 78% were correctly predicted.\n- Precision for \"heart disease\" is 0.89, meaning that out of all the instances predicted as \"heart disease,\" 89% were correctly predicted.\n\nThe accuracy of the Naive Bayes Classifier model is slightly higher than the Logistic Regression and SVM Classifier model (both at 0.813).",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Classifier Performance Comparison\n\n| Metric             | SVM     | Logistic Regression | Naive Bayes |\n|--------------------|---------|---------------------|-------------|\n| **Accuracy**       | 0.813   | 0.813               | 0.835       |\n| **Precision (No Disease)** | 0.80    | 0.80                | 0.78        |\n| **Precision (Heart Disease)** | 0.82    | 0.82                | 0.89        |\n| **Recall (No Disease)**    | 0.78    | 0.78                | 0.88        |\n| **Recall (Heart Disease)** | 0.84    | 0.84                | 0.80        |\n| **F1-Score (No Disease)**  | 0.79    | 0.79                | 0.83        |\n| **F1-Score (Heart Disease)** | 0.83    | 0.83                | 0.84        |\n\n### Summary of Findings:\n1. **Accuracy**:\n   - Both the SVM and Logistic Regression classifiers achieved an accuracy of approximately 81.3%.\n   - The Naive Bayes classifier outperformed the other two with an accuracy of approximately 83.5%.\n\n\n2. **Precision**:\n   - For predicting \"no disease\", SVM and Logistic Regression have higher precision (0.80) compared to Naive Bayes (0.78).\n   - For predicting \"heart disease\", Naive Bayes has the highest precision (0.89), significantly better than both SVM and Logistic Regression (0.82).\n\n\n3. **Recall**:\n   - For predicting \"no disease\", Naive Bayes achieves the highest recall (0.88), indicating it is very effective at identifying patients without heart disease.\n   - For predicting \"heart disease\", SVM and Logistic Regression have higher recall (0.84) compared to Naive Bayes (0.80).\n\n\n4. **F1-Score**:\n   - For predicting \"no disease\", Naive Bayes has the highest F1-score (0.83), indicating a good balance between precision and recall.\n   - For predicting \"heart disease\", the F1-scores are very close, with Naive Bayes slightly leading (0.84) over SVM and Logistic Regression (0.83).\n  \n### Next Steps:\n- Experimenting with more advanced machine learning models such as Random Forests, Gradient Boosting Machines (GBM), and XGBoost can provide insights into their performance compared to the current models.\n- Deep learning approaches using neural networks, specifically feedforward neural networks or more sophisticated architectures like convolutional neural networks (CNNs) and recurrent neural networks (RNNs), can be explored for possibly better accuracy and predictive power.\n- Conducting hyperparameter tuning using techniques such as grid search or randomized search can optimize model performance.\n- Implementing cross-validation will ensure that the results are robust and generalizable to new, unseen data.\n\n### Conclusion:\nBased on the comparison, the Naive Bayes classifier demonstrates superior performance overall, particularly in accuracy and precision for predicting heart disease. While SVM and Logistic Regression also perform well and consistently, they do not surpass Naive Bayes in key metrics. Therefore, for the dataset used, Naive Bayes is recommended as the best-performing classifier.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}